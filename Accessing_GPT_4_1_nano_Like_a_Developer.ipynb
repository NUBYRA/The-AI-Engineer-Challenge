{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of language model-powered applications, but they serve different purposes and have distinct features. Here's a comparison to help clarify their differences:\n",
            "\n",
            "**1. Purpose and Focus**\n",
            "\n",
            "- **LangChain:**\n",
            "  - **Primary Focus:** Building complex, composable language model applications, particularly those involving prompt management, chaining multiple steps, agents, and integrations.\n",
            "  - **Use Cases:** Conversational agents, question-answering systems, workflows that combine multiple models or tools, and general orchestrations involving language models.\n",
            "  - **Highlights:** Offers abstractions for prompt templates, chains (sequential or multi-step processes), memory, and integrations with various data sources and APIs.\n",
            "\n",
            "- **LlamaIndex (GPT Index):**\n",
            "  - **Primary Focus:** Facilitating the ingestion, indexing, and querying of large unstructured data sources to build effective retrieval-based question-answering systems.\n",
            "  - **Use Cases:** Data indexing from documents, PDFs, websites, and enabling efficient retrieval and answering via language models.\n",
            "  - **Highlights:** Provides tools for data ingestion, creating semantic indices, retrieval mechanisms, and connecting these to LLMs for question answering.\n",
            "\n",
            "**2. Core Functionality**\n",
            "\n",
            "- **LangChain:**\n",
            "  - Facilitates the construction of **multi-step workflows** involving prompts and model calls.\n",
            "  - Supports **agents** capable of deciding which action or tool to invoke based on user input.\n",
            "  - Provides integrations to various external tools, APIs, and data sources.\n",
            "  - Emphasizes **prompt engineering**, chain management, and application orchestration.\n",
            "\n",
            "- **LlamaIndex:**\n",
            "  - Focuses on **building semantic indices** over unstructured data.\n",
            "  - Provides tools for **data ingestion**, preprocessing, and **index creation**.\n",
            "  - Enables fast **retrieval-augmented generation (RAG)** workflows by combining data retrieval with language models.\n",
            "  - Supports multiple index types like tree-based, vector-based, or hybrid indexes.\n",
            "\n",
            "**3. Typical Use Cases**\n",
            "\n",
            "- **LangChain:**\n",
            "  - Building chatbots, virtual assistants, or workflow automation that require orchestrating multiple model calls or tools.\n",
            "  - Managing conversations with memory and context.\n",
            "  - Creating applications that combine language models with APIs or databases.\n",
            "\n",
            "- **LlamaIndex:**\n",
            "  - Creating knowledge bases from large document collections.\n",
            "  - Implementing retrieval-augmented question-answering systems.\n",
            "  - Efficiently managing large datasets for querying via LLMs.\n",
            "\n",
            "**4. Integration and Ecosystem**\n",
            "\n",
            "- **LangChain:**\n",
            "  - Supports a wide array of language models (OpenAI, Hugging Face, Azure) and external APIs.\n",
            "  - Modular design with an emphasis on prompting, chaining, and agent frameworks.\n",
            "\n",
            "- **LlamaIndex:**\n",
            "  - Integrates with various data storage formats and indexing structures.\n",
            "  - Often used in conjunction with OpenAI models but more focused on the data pipeline.\n",
            "\n",
            "---\n",
            "\n",
            "### **In summary:**\n",
            "\n",
            "| Aspect                  | **LangChain**                               | **LlamaIndex**                          |\n",
            "|-------------------------|----------------------------------------------|----------------------------------------|\n",
            "| Main Purpose            | Building complex LLM applications and workflows | Indexing and querying unstructured data for retrieval-augmented tasks |\n",
            "| Focus                   | Prompt management, chains, agents            | Data ingestion, semantic indexing, retrieval |\n",
            "| Typical Use Cases       | Conversational agents, automation workflows | Knowledge base creation, document Q&A  |\n",
            "| Ecosystem/Integrations  | Wide range of models, APIs, tools          | Data sources, indexing techniques      |\n",
            "\n",
            "**In practice,** these frameworks can be complementary: LangChain handles orchestrations and multi-step logic, while LlamaIndex provides the robust data indexing and retrieval capabilities for large document datasets. Many developers use them together to build sophisticated AI applications.\n"
          ]
        }
      ],
      "source": [
        "# Now your code will work\n",
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\": \"user\", \"content\": YOUR_PROMPT}]\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are two popular frameworks designed to facilitate building language model applications, but they serve different purposes and have distinct features. Here's a comparison outlining their key differences:\n",
              "\n",
              "**1. Purpose and Focus:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Primarily a framework for building applications that leverage language models (LLMs).\n",
              "  - Emphasizes composing complex workflows (chains), integrations with external tools, memory management, and routing.\n",
              "  - Suitable for creating chatbots, question-answering systems, agents, and multi-step pipelines.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focuses on enabling LLMs to efficiently access, index, and query large collections of external data (e.g., documents, databases).\n",
              "  - Designed for building semantic search, retrieval-augmented generation (RAG), and data ingestion pipelines.\n",
              "  - Simplifies connecting LLMs with structured and unstructured data sources.\n",
              "\n",
              "**2. Core Functionality:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Offers components like chains, agents, memory, and tools to orchestrate interactions with LLMs.\n",
              "  - Supports prompt management, conversation history, reasoning over multiple steps, and external API integration.\n",
              "  - Provides a flexible way to construct custom applications with modular blocks.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Provides data ingestion tools to create indices over various data formats (texts, PDFs, HTML, databases).\n",
              "  - Enables querying these indices with LLMs for fast, context-aware retrieval.\n",
              "  - Includes pre-built index types such as tree, list, and graph indices for different data structures.\n",
              "\n",
              "**3. Use Cases:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Building conversational agents, chatbots, multi-step reasoning systems.\n",
              "  - Creating applications that require calling external APIs, tools, or performing complex workflows.\n",
              "  - Managing dialogue state and memory.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Building knowledge bases from large documents.\n",
              "  - Developing retrieval-augmented question-answering systems.\n",
              "  - Indexing and querying large data repositories with LLMs for fast responses.\n",
              "\n",
              "**4. Integration and Extensibility:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Highly extensible with integrations to numerous LLM providers, APIs, tools, and custom components.\n",
              "  - Strong community support and a broad ecosystem.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focused on data ingestion and indexing; integrates with document storage solutions and supports various data formats.\n",
              "  - Can be combined with LLMs for querying, often used alongside LangChain for conversational contexts.\n",
              "\n",
              "**5. Typical Workflow:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Define prompts and chains.\n",
              "  - Assemble components into complex workflows.\n",
              "  - Manage conversation history and context.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Ingest and index data sources.\n",
              "  - Generate vector or semantic indices.\n",
              "  - Query indices to retrieve relevant information and pass it to the LLM for response generation.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary:**  \n",
              "- **Use LangChain** if you're building application logic, managing conversations, chaining multiple steps, or integrating with tools and APIs.  \n",
              "- **Use LlamaIndex** if your goal is to efficiently organize and query large collections of data to enhance retrieval-augmented generation capabilities.\n",
              "\n",
              "Often, these frameworks can be used together for building sophisticated AI applications—LlamaIndex can handle data indexing and retrieval, while LangChain orchestrates the overall application logic and user interactions."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time for this trivial question while I'm starving! Just give me whatever ice is available—crushed or cubed—so I can finally get some food!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I love both! Crushed ice is fantastic for cooling drinks quickly and gives a refreshing crunch, while cubed ice stays colder longer and looks elegant in a glass. It really depends on what I’m enjoying—like a slushy beverage versus a classic cocktail. Do you have a favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BmjrJcCVxgpqF3syaw7kVSw5bi9Dg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I love both! Crushed ice is fantastic for cooling drinks quickly and gives a refreshing crunch, while cubed ice stays colder longer and looks elegant in a glass. It really depends on what I’m enjoying—like a slushy beverage versus a classic cocktail. Do you have a favorite?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750955725, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=58, prompt_tokens=30, total_tokens=88, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Certainly! Here's a sentence using the words 'stimple' and 'falbean':\n",
              "\n",
              "\"During the quirky town festival, the stimple decorations and falbean costumes made the parade truly unforgettable.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench efficiently turned the falbean to secure the heavy machinery in place."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze Billy's options step by step.\n",
              "\n",
              "**Current situation:**\n",
              "- Local time: 1 PM\n",
              "- Goal: arrive home **before 7 PM EDT**\n",
              "\n",
              "**Assumptions:**\n",
              "- Billy's current local time corresponds to San Francisco time (Pacific Daylight Time, PDT)\n",
              "- The destination time zone (East Coast) is EDT\n",
              "- We need to consider time zone differences\n",
              "\n",
              "**Time zone details:**\n",
              "- PDT (Pacific) is typically UTC-7 or UTC-8 depending on daylight saving\n",
              "- EDT (Eastern) is UTC-4 or UTC-5 during daylight saving\n",
              "- Let's assume standard daylight saving times:\n",
              "  - PDT: UTC-7\n",
              "  - EDT: UTC-4\n",
              "\n",
              "**Calculating current time in EDT:**\n",
              "- Since it's 1 PM local (PDT), convert to EDT:\n",
              "  \n",
              "  EDT time = PDT time + 3 hours\n",
              "  \n",
              "  1 PM PDT + 3 hours = 4 PM EDT\n",
              "\n",
              "**Remaining time until 7 PM EDT:**\n",
              "- From 4 PM EDT to 7 PM EDT is **3 hours**\n",
              "\n",
              "---\n",
              "\n",
              "### Option 1: Fly + Bus\n",
              "- Fly time: 3 hours\n",
              "- Bus time after flight: 2 hours\n",
              "- Total travel time: 3 + 2 = **5 hours**\n",
              "\n",
              "**Total time from now:**\n",
              "- Start at 1 PM PDT (which is 4 PM EDT)\n",
              "- Add 5 hours: 4 PM + 5 hours = **9 PM EDT**\n",
              "\n",
              "**Result:**\n",
              "- Billy would arrive **at 9 PM EDT**, which is **after** the 7 PM deadline.\n",
              "\n",
              "---\n",
              "\n",
              "### Option 2: Teleporter + Bus\n",
              "- Teleporter: 0 hours\n",
              "- Bus: 1 hour\n",
              "- Total time: 1 hour\n",
              "\n",
              "**Total time from now:**\n",
              "- Starting at 1 PM PDT (4 PM EDT)\n",
              "- Add 1 hour: 4 PM + 1 hour = **5 PM EDT**\n",
              "\n",
              "**Result:**\n",
              "- Billy would arrive **at 5 PM EDT**, which is **before** the 7 PM deadline.\n",
              "\n",
              "---\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "**Yes, it matters which option Billy chooses.**\n",
              "\n",
              "- If he takes the **fly + bus** option, he will arrive after the deadline (9 PM), missing his goal.\n",
              "- If he takes the **teleporter + bus** option, he will arrive on time (5 PM), meeting his goal.\n",
              "\n",
              "**Therefore, Billy should opt for the teleporter + bus to arrive before 7 PM EDT.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's clarify the situation step by step:\n",
              "\n",
              "1. **Current time:**\n",
              "   - It's 1PM local time.\n",
              "   \n",
              "2. **Destination time requirement:**\n",
              "   - Billy needs to be home **before 7PM EDT**.\n",
              "\n",
              "3. **Understanding time zones:**\n",
              "   - San Francisco is in PDT (Pacific Daylight Time), which is UTC-7.\n",
              "   - The destination is on EDT (Eastern Daylight Time), which is UTC-4.\n",
              "   \n",
              "   Therefore:\n",
              "   - **Time difference:** EDT is 3 hours ahead of PDT.\n",
              "   - If it's 1PM in San Francisco (PDT), then **it's 4PM EDT** at that same moment.\n",
              "\n",
              "4. **Calculating the deadline:**\n",
              "   - Billy needs to arrive **before 7PM EDT**.\n",
              "   - It is currently **4PM EDT** at the start.\n",
              "\n",
              "5. **Travel options:**\n",
              "\n",
              "   **Option A:**\n",
              "   - Fly (3 hours) + Bus (2 hours) = total **5 hours**.\n",
              "\n",
              "   **Option B:**\n",
              "   - Teleporter (0 hours) + Bus (1 hour) = total **1 hour**.\n",
              "\n",
              "6. **Estimating arrival times:**\n",
              "\n",
              "   **For Option A:**\n",
              "   - Depart at 1PM local (4PM EDT).\n",
              "   - Flight takes 3 hours → arrive at 4PM + 3 hours = 7PM EDT.\n",
              "   - Bus takes 2 hours → arrive at 7PM + 2 hours = **9PM EDT**.\n",
              "\n",
              "   **For Option B:**\n",
              "   - Teleporter is instantaneous, so remain at 4PM EDT.\n",
              "   - Bus takes 1 hour → arrive at 4PM + 1 hour = **5PM EDT**.\n",
              "\n",
              "7. **Final assessment:**\n",
              "\n",
              "   - **Option A:** Billy arrives at 9PM EDT, which is **after** the 7PM deadline.\n",
              "   - **Option B:** Billy arrives at 5PM EDT, which is **before** the 7PM deadline.\n",
              "\n",
              "**Conclusion:**\n",
              "\n",
              "- Since the goal is to get home **before 7PM EDT**, only **Option B** (teleporter + bus) allows Billy to reach on time.\n",
              "- **Therefore, it does matter which option Billy selects.** The teleporter plus bus option is necessary to arrive before the deadline.\n",
              "\n",
              "---\n",
              "\n",
              "**Answer:** Yes, it matters. To arrive before 7PM EDT, Billy should take the teleporter (instant) plus the bus (1 hour), enabling him to arrive at 5PM EDT, whereas flying and then bus would arrive too late at 9PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "activate",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
